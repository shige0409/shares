{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"neural_machine_translation_with_transformer のコピー","provenance":[{"file_id":"https://github.com/keras-team/keras-io/blob/master/examples/nlp/ipynb/neural_machine_translation_with_transformer.ipynb","timestamp":1627341695250}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"JtC6ayDKHGT4"},"source":["# English-to-Spanish translation with a sequence-to-sequence Transformer\n","\n","**Author:** [fchollet](https://twitter.com/fchollet)<br>\n","**Date created:** 2021/05/26<br>\n","**Last modified:** 2021/05/26<br>\n","**Description:** Implementing a sequence-to-sequene Transformer and training it on a machine translation task."]},{"cell_type":"markdown","metadata":{"id":"csXEpIDxHGT5"},"source":["## Introduction\n","\n","In this example, we'll build a sequence-to-sequence Transformer model, which\n","we'll train on an English-to-Spanish machine translation task.\n","\n","You'll learn how to:\n","\n","- Vectorize text using the Keras `TextVectorization` layer.\n","- Implement a `TransformerEncoder` layer, a `TransformerDecoder` layer,\n","and a `PositionalEmbedding` layer.\n","- Prepare data for training a sequence-to-sequence model.\n","- Use the trained model to generate translations of never-seen-before\n","input sentences (sequence-to-sequence inference).\n","\n","The code featured here is adapted from the book\n","[Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition)\n","(chapter 11: Deep learning for text).\n","The present example is fairly barebones, so for detailed explanations of\n","how each building block works, as well as the theory behind Transformers,\n","I recommend reading the book."]},{"cell_type":"markdown","metadata":{"id":"8IEJV4aKHGT6"},"source":["## Setup"]},{"cell_type":"code","metadata":{"id":"9RbGlxMMHGT6","executionInfo":{"status":"ok","timestamp":1627376325028,"user_tz":-540,"elapsed":823,"user":{"displayName":"kiyoshige goto","photoUrl":"","userId":"01082649767351199152"}}},"source":["import pathlib\n","import random\n","import string\n","import re\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","# from tensorflow.keras.preprocessing.text import Tokenizer\n","from keras.layers.experimental.preprocessing import TextVectorization"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"ISleq1qMEWYA","executionInfo":{"status":"ok","timestamp":1627376325790,"user_tz":-540,"elapsed":2,"user":{"displayName":"kiyoshige goto","photoUrl":"","userId":"01082649767351199152"}}},"source":["text_file = keras.utils.get_file(\n","    fname=\"spa-eng.zip\",\n","    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\",\n","    extract=True,\n",")\n","text_file = pathlib.Path(text_file).parent / \"spa-eng\" / \"spa.txt\""],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"adAWaF4NHGT6"},"source":["## Parsing the data\n","\n","Each line contains an English sentence and its corresponding Spanish sentence.\n","The English sentence is the *source sequence* and Spanish one is the *target sequence*.\n","We prepend the token `\"[start]\"` and we append the token `\"[end]\"` to the Spanish sentence."]},{"cell_type":"code","metadata":{"id":"5PhuqSlyHGT7","executionInfo":{"status":"ok","timestamp":1627376326570,"user_tz":-540,"elapsed":2,"user":{"displayName":"kiyoshige goto","photoUrl":"","userId":"01082649767351199152"}}},"source":["with open(text_file) as f:\n","    lines = f.read().split(\"\\n\")[:-1]\n","text_pairs = []\n","for line in lines:\n","    eng, spa = line.split(\"\\t\")\n","    spa = \"[start] \" + spa + \" [end]\"\n","    text_pairs.append((eng, spa))"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4O6f_zpAHGT7"},"source":["Here's what our sentence pairs look like:"]},{"cell_type":"code","metadata":{"id":"GnqXv4dJHGT7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627376328179,"user_tz":-540,"elapsed":7,"user":{"displayName":"kiyoshige goto","photoUrl":"","userId":"01082649767351199152"}},"outputId":"9a9587b2-c2ec-4abf-dbad-e2f387e9fb05"},"source":["for _ in range(5):\n","    print(random.choice(text_pairs))"],"execution_count":18,"outputs":[{"output_type":"stream","text":["('No one could solve the problem after all.', '[start] A fin de cuentas, nadie pudo resolver el problema. [end]')\n","('When will you return?', '[start] ¿Cuándo volverás? [end]')\n","('I know how to get things done.', '[start] Sé como hacer que las cosas se hagan. [end]')\n","(\"The world's largest telescope is in the Canary Islands.\", '[start] El telescopio más grande del mundo se encuentra en las Islas Canarias. [end]')\n","(\"You don't realize its value until you have lost your health.\", '[start] No aprecias tu salud hasta que la pierdes. [end]')\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rE9qJK72HGT7"},"source":["Now, let's split the sentence pairs into a training set, a validation set,\n","and a test set."]},{"cell_type":"code","metadata":{"id":"W6EPj6mkHGT7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627376329668,"user_tz":-540,"elapsed":847,"user":{"displayName":"kiyoshige goto","photoUrl":"","userId":"01082649767351199152"}},"outputId":"9032fb5e-1ca3-491b-c325-966e91757bc9"},"source":["random.shuffle(text_pairs)\n","num_val_samples = int(0.15 * len(text_pairs))\n","num_train_samples = len(text_pairs) - 2 * num_val_samples\n","train_pairs = text_pairs[:num_train_samples]\n","val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n","test_pairs = text_pairs[num_train_samples + num_val_samples :]\n","\n","print(f\"{len(text_pairs)} total pairs\")\n","print(f\"{len(train_pairs)} training pairs\")\n","print(f\"{len(val_pairs)} validation pairs\")\n","print(f\"{len(test_pairs)} test pairs\")"],"execution_count":19,"outputs":[{"output_type":"stream","text":["118964 total pairs\n","83276 training pairs\n","17844 validation pairs\n","17844 test pairs\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KW82GOwXHGT7"},"source":["## Vectorizing the text data\n","\n","We'll use two instances of the `TextVectorization` layer to vectorize the text\n","data (one for English and one for Spanish),\n","that is to say, to turn the original strings into integer sequences\n","where each integer represents the index of a word in a vocabulary.\n","\n","The English layer will use the default string standardization (strip punctuation characters)\n","and splitting scheme (split on whitespace), while\n","the Spanish layer will use a custom standardization, where we add the character\n","`\"¿\"` to the set of punctuation characters to be stripped.\n","\n","Note: in a production-grade machine translation model, I would not recommend\n","stripping the punctuation characters in either language. Instead, I would recommend turning\n","each punctuation character into its own token,\n","which you could achieve by providing a custom `split` function to the `TextVectorization` layer."]},{"cell_type":"code","metadata":{"id":"4O-_SUedHGT7","executionInfo":{"status":"ok","timestamp":1627376371137,"user_tz":-540,"elapsed":41471,"user":{"displayName":"kiyoshige goto","photoUrl":"","userId":"01082649767351199152"}}},"source":["strip_chars = string.punctuation + \"¿\"\n","strip_chars = strip_chars.replace(\"[\", \"\")\n","strip_chars = strip_chars.replace(\"]\", \"\")\n","\n","vocab_size = 15000\n","sequence_length = 20\n","batch_size = 64\n","\n","\n","def custom_standardization(input_string):\n","    lowercase = tf.strings.lower(input_string)\n","    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n","\n","\n","eng_vectorization = TextVectorization(\n","    max_tokens=vocab_size, output_mode=\"int\", output_sequence_length=sequence_length,\n",")\n","spa_vectorization = TextVectorization(\n","    max_tokens=vocab_size,\n","    output_mode=\"int\",\n","    output_sequence_length=sequence_length + 1,\n","    standardize=custom_standardization,\n",")\n","train_eng_texts = [pair[0] for pair in train_pairs]\n","train_spa_texts = [pair[1] for pair in train_pairs]\n","eng_vectorization.adapt(train_eng_texts)\n","spa_vectorization.adapt(train_spa_texts)"],"execution_count":20,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4CyeUTdaHGT8"},"source":["Next, we'll format our datasets.\n","\n","At each training step, the model will seek to predict target words N+1 (and beyond)\n","using the source sentence and the target words 0 to N.\n","\n","As such, the training dataset will yield a tuple `(inputs, targets)`, where:\n","\n","- `inputs` is a dictionary with the keys `encoder_inputs` and `decoder_inputs`.\n","`encoder_inputs` is the vectorized source sentence and `encoder_inputs` is the target sentence \"so far\",\n","that is to say, the words 0 to N used to predict word N+1 (and beyond) in the target sentence.\n","- `target` is the target sentence offset by one step:\n","it provides the next words in the target sentence -- what the model will try to predict."]},{"cell_type":"code","metadata":{"id":"3ZiIZBDpHGT8","executionInfo":{"status":"ok","timestamp":1627376372109,"user_tz":-540,"elapsed":992,"user":{"displayName":"kiyoshige goto","photoUrl":"","userId":"01082649767351199152"}}},"source":["\n","def format_dataset(eng, spa):\n","    eng = eng_vectorization(eng)\n","    spa = spa_vectorization(spa)\n","    return ({\"encoder_inputs\": eng, \"decoder_inputs\": spa[:, :-1],}, spa[:, 1:])\n","\n","\n","def make_dataset(pairs):\n","    eng_texts, spa_texts = zip(*pairs)\n","    eng_texts = list(eng_texts)\n","    spa_texts = list(spa_texts)\n","    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n","    dataset = dataset.batch(batch_size)\n","    dataset = dataset.map(format_dataset)\n","    return dataset.shuffle(2048).prefetch(16).cache()\n","\n","\n","train_ds = make_dataset(train_pairs)\n","val_ds = make_dataset(val_pairs)"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jUfP3QmNMER2","executionInfo":{"status":"ok","timestamp":1627376395997,"user_tz":-540,"elapsed":240,"user":{"displayName":"kiyoshige goto","photoUrl":"","userId":"01082649767351199152"}},"outputId":"b2ec0d6a-f5bc-40a6-a65a-09085c5329d0"},"source":["eng_vectorization(train_eng_texts[0:2])"],"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(2, 20), dtype=int64, numpy=\n","array([[  5, 143,  87,  42,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0],\n","       [  6,  44,  62, 117, 230,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0]])>"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"code","metadata":{"id":"QJcdIWA5SeuO","executionInfo":{"status":"ok","timestamp":1627378039964,"user_tz":-540,"elapsed":246,"user":{"displayName":"kiyoshige goto","photoUrl":"","userId":"01082649767351199152"}},"outputId":"b984060b-eb7e-4cbe-fbd7-a0d0b82303b4","colab":{"base_uri":"https://localhost:8080/"}},"source":["spa_vectorization(train_spa_texts[0:2])"],"execution_count":36,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(2, 21), dtype=int64, numpy=\n","array([[   2,  307, 3480,    3,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n","       [   2,  100, 1170,  208,    3,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0]])>"]},"metadata":{"tags":[]},"execution_count":36}]},{"cell_type":"markdown","metadata":{"id":"Qa3PGUnhHGT8"},"source":["Let's take a quick look at the sequence shapes\n","(we have batches of 64 pairs, and all sequences are 20 steps long):"]},{"cell_type":"code","metadata":{"id":"m5TTLkYxHGT8"},"source":["for inputs, targets in train_ds.take(1):\n","    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n","    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n","    print(f\"targets.shape: {targets.shape}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ReEzJD3GQTqi","executionInfo":{"status":"ok","timestamp":1627377495472,"user_tz":-540,"elapsed":222,"user":{"displayName":"kiyoshige goto","photoUrl":"","userId":"01082649767351199152"}},"outputId":"111406c1-fe38-489f-93e0-ae4d7b9393fa"},"source":["inputs[\"decoder_inputs\"][0]"],"execution_count":35,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(20,), dtype=int64, numpy=\n","array([   2,   32, 2464,  309, 5446,    3,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0])>"]},"metadata":{"tags":[]},"execution_count":35}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":168},"id":"GhA_cvb3QHpK","executionInfo":{"status":"error","timestamp":1627377439835,"user_tz":-540,"elapsed":921,"user":{"displayName":"kiyoshige goto","photoUrl":"","userId":"01082649767351199152"}},"outputId":"9ace161e-1044-486f-d99f-af7e06f678c2"},"source":["inputs, targets = train_ds.take(1)"],"execution_count":31,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-31-8ba9aba6e794>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"]}]},{"cell_type":"markdown","metadata":{"id":"u--7nURZHGT8"},"source":["## Building the model\n","\n","Our sequence-to-sequence Transformer consists of a `TransformerEncoder`\n","and a `TransformerDecoder` chained together. To make the model aware of word order,\n","we also use a `PositionalEmbedding` layer.\n","\n","The source sequence will be pass to the `TransformerEncoder`,\n","which will produce a new representation of it.\n","This new representation will then be passed\n","to the `TransformerDecoder`, together with the target sequence so far (target words 0 to N).\n","The `TransformerDecoder` will then seek to predict the next words in the target sequence (N+1 and beyond).\n","\n","A key detail that makes this possible is causal masking\n","(see method `get_causal_attention_mask()` on the `TransformerDecoder`).\n","The `TransformerDecoder` sees the entire sequences at once, and thus we must make\n","sure that it only uses information from target tokens 0 to N when predicting token N+1\n","(otherwise, it could use information from the future, which would\n","result in a model that cannot be used at inference time)."]},{"cell_type":"code","metadata":{"id":"Q29K-Da-HGT8"},"source":["\n","class TransformerEncoder(layers.Layer):\n","    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n","        super(TransformerEncoder, self).__init__(**kwargs)\n","        self.embed_dim = embed_dim\n","        self.dense_dim = dense_dim\n","        self.num_heads = num_heads\n","        self.attention = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim\n","        )\n","        self.dense_proj = keras.Sequential(\n","            [layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n","        )\n","        self.layernorm_1 = layers.LayerNormalization()\n","        self.layernorm_2 = layers.LayerNormalization()\n","        self.supports_masking = True\n","\n","    def call(self, inputs, mask=None):\n","        if mask is not None:\n","            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype=\"int32\")\n","        attention_output = self.attention(\n","            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n","        )\n","        proj_input = self.layernorm_1(inputs + attention_output)\n","        proj_output = self.dense_proj(proj_input)\n","        return self.layernorm_2(proj_input + proj_output)\n","\n","\n","class PositionalEmbedding(layers.Layer):\n","    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n","        super(PositionalEmbedding, self).__init__(**kwargs)\n","        self.token_embeddings = layers.Embedding(\n","            input_dim=vocab_size, output_dim=embed_dim\n","        )\n","        self.position_embeddings = layers.Embedding(\n","            input_dim=sequence_length, output_dim=embed_dim\n","        )\n","        self.sequence_length = sequence_length\n","        self.vocab_size = vocab_size\n","        self.embed_dim = embed_dim\n","\n","    def call(self, inputs):\n","        length = tf.shape(inputs)[-1]\n","        positions = tf.range(start=0, limit=length, delta=1)\n","        embedded_tokens = self.token_embeddings(inputs)\n","        embedded_positions = self.position_embeddings(positions)\n","        return embedded_tokens + embedded_positions\n","\n","    def compute_mask(self, inputs, mask=None):\n","        return tf.math.not_equal(inputs, 0)\n","\n","\n","class TransformerDecoder(layers.Layer):\n","    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n","        super(TransformerDecoder, self).__init__(**kwargs)\n","        self.embed_dim = embed_dim\n","        self.latent_dim = latent_dim\n","        self.num_heads = num_heads\n","        self.attention_1 = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim\n","        )\n","        self.attention_2 = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim\n","        )\n","        self.dense_proj = keras.Sequential(\n","            [layers.Dense(latent_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n","        )\n","        self.layernorm_1 = layers.LayerNormalization()\n","        self.layernorm_2 = layers.LayerNormalization()\n","        self.layernorm_3 = layers.LayerNormalization()\n","        self.supports_masking = True\n","\n","    def call(self, inputs, encoder_outputs, mask=None):\n","        causal_mask = self.get_causal_attention_mask(inputs)\n","        if mask is not None:\n","            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n","            padding_mask = tf.minimum(padding_mask, causal_mask)\n","\n","        attention_output_1 = self.attention_1(\n","            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n","        )\n","        out_1 = self.layernorm_1(inputs + attention_output_1)\n","\n","        attention_output_2 = self.attention_2(\n","            query=out_1,\n","            value=encoder_outputs,\n","            key=encoder_outputs,\n","            attention_mask=padding_mask,\n","        )\n","        out_2 = self.layernorm_2(out_1 + attention_output_2)\n","\n","        proj_output = self.dense_proj(out_2)\n","        return self.layernorm_3(out_2 + proj_output)\n","\n","    def get_causal_attention_mask(self, inputs):\n","        input_shape = tf.shape(inputs)\n","        batch_size, sequence_length = input_shape[0], input_shape[1]\n","        i = tf.range(sequence_length)[:, tf.newaxis]\n","        j = tf.range(sequence_length)\n","        mask = tf.cast(i >= j, dtype=\"int32\")\n","        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n","        mult = tf.concat(\n","            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n","            axis=0,\n","        )\n","        return tf.tile(mask, mult)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Cpe4jyBPHGT9"},"source":["Next, we assemble the end-to-end model."]},{"cell_type":"code","metadata":{"id":"l9X8d9BTHGT9"},"source":["embed_dim = 128\n","latent_dim = 256\n","num_heads = 2\n","\n","encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n","x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n","encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n","encoder = keras.Model(encoder_inputs, encoder_outputs)\n","\n","decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n","encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n","x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n","x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n","x = layers.Dropout(0.5)(x)\n","decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n","decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n","\n","decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n","transformer = keras.Model(\n","    [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yuZmPAa0HGT9"},"source":["## Training our model\n","\n","We'll use accuracy as a quick way to monitor training progress on the validation data.\n","Note that machine translation typically uses BLEU scores as well as other metrics, rather than accuracy.\n","\n","Here we only train for 1 epoch, but to get the model to actually converge\n","you should train for at least 30 epochs."]},{"cell_type":"code","metadata":{"id":"MTxtnngRHGT9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627341852665,"user_tz":-540,"elapsed":78266,"user":{"displayName":"kiyoshige goto","photoUrl":"","userId":"01082649767351199152"}},"outputId":"f7cd6cbd-b084-469c-963c-921d0b2b15ec"},"source":["epochs = 1  # This should be at least 30 for convergence\n","\n","transformer.summary()\n","transformer.compile(\n","    \"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",")\n","transformer.fit(train_ds, epochs=epochs, validation_data=val_ds)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"transformer\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","encoder_inputs (InputLayer)     [(None, None)]       0                                            \n","__________________________________________________________________________________________________\n","positional_embedding (Positiona (None, None, 128)    1922560     encoder_inputs[0][0]             \n","__________________________________________________________________________________________________\n","decoder_inputs (InputLayer)     [(None, None)]       0                                            \n","__________________________________________________________________________________________________\n","transformer_encoder (Transforme (None, None, 128)    198400      positional_embedding[0][0]       \n","__________________________________________________________________________________________________\n","model_1 (Functional)            (None, None, 15000)  4188184     decoder_inputs[0][0]             \n","                                                                 transformer_encoder[0][0]        \n","==================================================================================================\n","Total params: 6,309,144\n","Trainable params: 6,309,144\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","1302/1302 [==============================] - 78s 53ms/step - loss: 1.6796 - accuracy: 0.4206 - val_loss: 1.3530 - val_accuracy: 0.5046\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f978cf41f90>"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"mhCFVIqnHGT9"},"source":["## Decoding test sentences\n","\n","Finally, let's demonstrate how to translate brand new English sentences.\n","We simply feed into the model the vectorized English sentence\n","as well as the target token `\"[start]\"`, then we repeatedly generated the next token, until\n","we hit the token `\"[end]\"`."]},{"cell_type":"code","metadata":{"id":"EKknAD-cHGT9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627341981888,"user_tz":-540,"elapsed":1115,"user":{"displayName":"kiyoshige goto","photoUrl":"","userId":"01082649767351199152"}},"outputId":"5bad6783-fe4a-4bea-9f8a-dd0a7ea289f7"},"source":["spa_vocab = spa_vectorization.get_vocabulary()\n","spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n","max_decoded_sentence_length = 20\n","\n","\n","def decode_sequence(input_sentence):\n","    tokenized_input_sentence = eng_vectorization([input_sentence])\n","    decoded_sentence = \"[start]\"\n","    for i in range(max_decoded_sentence_length):\n","        tokenized_target_sentence = spa_vectorization([decoded_sentence])[:, :-1]\n","        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n","\n","        sampled_token_index = np.argmax(predictions[0, i, :])\n","        sampled_token = spa_index_lookup[sampled_token_index]\n","        decoded_sentence += \" \" + sampled_token\n","\n","        if sampled_token == \"[end]\":\n","            break\n","    return decoded_sentence\n","\n","\n","test_eng_texts = [pair[0] for pair in test_pairs]\n","for _ in range(5):\n","    input_sentence = random.choice(test_eng_texts)\n","    translated = decode_sequence(input_sentence)\n","    print(input_sentence, \"  =>. \", translated)\n","    print(\"------------\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["I like potato salad.   =>.  [start] me gusta la playa [end]\n","------------\n","I share everything with Tom.   =>.  [start] yo [UNK] con todo [end]\n","------------\n","Tom doesn't like the way Mary treats John.   =>.  [start] tom no le gusta la puerta a mary [end]\n","------------\n","I appreciate your help.   =>.  [start] yo necesito tu nombre [end]\n","------------\n","Weeds sprang up in the garden.   =>.  [start] [UNK] a las dos en el tren [end]\n","------------\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"74RfcsSkIwLs"},"source":[""],"execution_count":null,"outputs":[]}]}